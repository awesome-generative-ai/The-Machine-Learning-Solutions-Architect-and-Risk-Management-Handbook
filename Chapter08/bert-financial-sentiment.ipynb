{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b52387",
   "metadata": {},
   "source": [
    "# bert-financial-sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0619ea",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ef4b66-0322-4fc1-982f-fc72787e29e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 pip3 工具安装 transformers 库，它提供了各种预训练模型、tokenizer 和其他工具，方便用户进行自然语言处理任务。\n",
    "!pip install transformers \n",
    "\n",
    "# 使用 pip3 工具安装 ipywidgets 库，它提供了一系列交互式小部件，用于在 Jupyter Notebook 中创建动态和交互式的用户界面。\n",
    "!pip install ipywidgets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f5ebfe",
   "metadata": {},
   "source": [
    "**解释：**\n",
    "\n",
    "这段代码使用 `pip3` 工具安装了两个 Python 库： `transformers` 和 `ipywidgets`。\n",
    "\n",
    "* **`transformers`** 库： 用于自然语言处理 (NLP) 的一个强大工具包，提供各种预训练模型、tokenizer 和其他工具，方便用户进行 NLP 任务，例如文本分类、文本生成、机器翻译等。它包含了各种预训练模型，例如 BERT、GPT-2、RoBERTa 等等，可以根据用户的任务需求选择合适的模型进行微调或使用。\n",
    "* **`ipywidgets`** 库： 用于在 Jupyter Notebook 环境中提供交互式小部件，让用户能够通过鼠标或键盘与代码进行交互。它可以用来创建动态和交互式的用户界面，提高代码的可视化效果，并使代码更易于调试和理解。\n",
    "\n",
    "这段代码的主要作用是安装 `transformers` 和 `ipywidgets` 两个库，为后续使用这些库进行 NLP 任务和 Jupyter Notebook 交互式操作做好准备。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1726da7",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9368d06b-c1a8-4fba-8b9e-e32a5e8af88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 logging 库，用于记录程序运行过程中的信息和错误。\n",
    "import logging \n",
    "\n",
    "# 导入 os 库，用于操作文件系统。\n",
    "import os \n",
    "\n",
    "# 导入 sys 库，用于访问系统特定的参数和函数。\n",
    "import sys \n",
    "\n",
    "# 导入 numpy 库，用于处理数组和矩阵运算。\n",
    "import numpy as np \n",
    "\n",
    "# 导入 pandas 库，用于读取和处理数据表格。\n",
    "import pandas as pd \n",
    "\n",
    "# 导入 PyTorch 库，用于构建和训练深度学习模型。\n",
    "import torch \n",
    "\n",
    "# 导入 PyTorch 的 DataLoader 和 TensorDataset 类，用于构建数据加载器和张量数据集。\n",
    "from torch.utils.data import DataLoader, TensorDataset \n",
    "\n",
    "# 导入 Hugging Face 的 transformers 库的 AdamW、BertForSequenceClassification 和 BertTokenizer 类，\n",
    "# 用于使用预训练的 BERT 模型进行文本分类任务。\n",
    "from transformers import AdamW, BertForSequenceClassification, BertTokenizer \n",
    "\n",
    "# 导入 scikit-learn 的 OrdinalEncoder 类，用于将分类特征转换为数值型特征。\n",
    "from sklearn.preprocessing import OrdinalEncoder \n",
    "\n",
    "# 导入 scikit-learn 的 train_test_split 函数，用于将数据集分成训练集和测试集。\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# 导入 SimpleNamespace 类，用于创建命名空间对象，方便组织代码结构。\n",
    "from types import SimpleNamespace \n",
    "\n",
    "# 创建一个名为 __name__ 的日志记录器。\n",
    "logger = logging.getLogger(__name__) \n",
    "\n",
    "# 设置日志记录器的级别为 DEBUG，即记录所有级别的信息。\n",
    "logger.setLevel(logging.DEBUG) \n",
    "\n",
    "# 将日志信息输出到标准输出流。\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e25ec4",
   "metadata": {},
   "source": [
    "**解释：**\n",
    "\n",
    "这段代码导入了所需的库，并创建了一个日志记录器，用于在程序运行过程中记录信息和错误。\n",
    "\n",
    "* **`logging` 库：**  用于在程序中记录信息和错误，方便调试和跟踪程序运行过程。\n",
    "* **`os` 库：** 用于与操作系统交互，访问文件系统、环境变量等。\n",
    "* **`sys` 库：**  提供与 Python 解释器相关的信息和函数，例如访问命令行参数、获取系统平台信息等。\n",
    "* **`numpy` 库：**  用于处理数组和矩阵运算，提供高效的数值计算功能。\n",
    "* **`pandas` 库：**  用于读取和处理数据表格，提供灵活的数据操作和分析功能。\n",
    "* **`torch` 库：**  用于构建和训练深度学习模型，提供了张量运算、自动微分、神经网络等功能。\n",
    "* **`DataLoader` 和 `TensorDataset` 类：** PyTorch 中的用于构建数据加载器和张量数据集，方便模型训练时加载数据。\n",
    "* **`transformers` 库：** Hugging Face 提供的用于自然语言处理任务的库，包含各种预训练模型、tokenizer 和其他工具，方便用户快速构建和使用 NLP 模型。\n",
    "* **`AdamW`、`BertForSequenceClassification` 和 `BertTokenizer` 类：**  `transformers` 库中用于使用预训练 BERT 模型进行文本分类任务的工具。\n",
    "* **`OrdinalEncoder` 类：**  `sklearn` 库中用于将分类特征转换为数值型特征。\n",
    "* **`train_test_split` 函数：**  `sklearn` 库中用于将数据集分成训练集和测试集。\n",
    "* **`SimpleNamespace` 类：**  用于创建命名空间对象，方便组织代码结构，将相关的变量和函数归类到同一个对象下。\n",
    "* **日志记录器：** 创建一个名为 `__name__` 的日志记录器，并设置其日志级别为 `DEBUG`，将日志信息输出到标准输出流。\n",
    "\n",
    "这段代码的主要作用是导入必要的库，并设置日志记录器，方便开发者调试和跟踪程序运行过程。后续的代码将会使用这些库中的工具进行自然语言处理和模型训练等任务。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a738160",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f7751b-bab2-4e61-8a9d-607636e2afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置数据集文件路径。\n",
    "filepath = './data/all-data.csv' \n",
    "\n",
    "# 读取数据集文件，并使用 ISO-8859-1 编码进行解码，\n",
    "# 只读取第一列和第二列数据，并分别命名为 \"sentiment\" 和 \"article\"。\n",
    "data = pd.read_csv(filepath, encoding=\"ISO-8859-1\", \n",
    "header=None, usecols=[0, 1], \n",
    "names=[\"sentiment\", \"article\"]) \n",
    "\n",
    "# 创建一个 OrdinalEncoder 对象，用于将分类特征转换为数值型特征。\n",
    "ord_enc = OrdinalEncoder() \n",
    "\n",
    "# 使用 OrdinalEncoder 对 \"sentiment\" 列进行编码，并将其存储在 \"sentiment\" 列中。\n",
    "data[\"sentiment\"] = ord_enc.fit_transform(data[[\"sentiment\"]]) \n",
    "\n",
    "# 将 \"sentiment\" 列的数据类型转换为整数类型。\n",
    "data = data.astype({'sentiment':'int'}) \n",
    "\n",
    "# 将数据集分成训练集和测试集，并使用默认比率进行分割。\n",
    "train, test = train_test_split(data) \n",
    "\n",
    "# 将训练集保存到文件 \"train.csv\"，不保存索引信息。\n",
    "train.to_csv(\"./data/train.csv\", index=False)\n",
    "\n",
    "# 将测试集保存到文件 \"test.csv\"，不保存索引信息。\n",
    "test.to_csv(\"./data/test.csv\", index=False)\n",
    "\n",
    "# 计算所有文本的最大长度，并将其存储在 MAX_LEN 变量中。\n",
    "MAX_LEN = data.article.str.len().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396b5c7",
   "metadata": {},
   "source": [
    "**解释：**\n",
    "\n",
    "这段代码完成以下几项工作：\n",
    "\n",
    "1. **读取数据：** 从 `all-data.csv` 文件中读取数据，将第一列作为情感标签（`sentiment`），第二列作为文本内容（`article`）。\n",
    "2. **预处理数据：**  使用 `OrdinalEncoder` 将情感标签转换成数值型特征，将情感类别转换为整数类型，并使用 `train_test_split` 函数将数据分成训练集和测试集。\n",
    "3. **保存数据：**  将训练集和测试集分别保存到`train.csv`和`test.csv`文件中。\n",
    "4. **计算文本最大长度：** 计算所有文本内容的最大长度，并将其存储在 `MAX_LEN` 变量中，用于后续模型训练时的文本截断和填充。\n",
    "\n",
    "**代码逻辑：**\n",
    "\n",
    "这段代码使用`pandas`库读取和处理`all-data.csv`文件，并使用`scikit-learn`库对数据进行预处理，最后将数据分割成训练集和测试集，并存储到相应的 CSV 文件中。\n",
    "\n",
    "* `pd.read_csv(filepath, encoding=\"ISO-8859-1\", header=None, usecols=[0, 1], names=[\"sentiment\", \"article\"])`：读取 CSV 文件，设置编码方式、不使用标题行、只读取第一列和第二列，并分别命名为 \"sentiment\" 和 \"article\"。\n",
    "* `ord_enc.fit_transform(data[[\"sentiment\"]])`：使用 OrdinalEncoder 将 \"sentiment\" 列进行编码，并将结果存储在 \"sentiment\" 列中。\n",
    "* `data.astype({'sentiment':'int'})`：将 \"sentiment\" 列的数据类型转换成整数类型。\n",
    "* `train, test = train_test_split(data)`：将数据集分成训练集和测试集。\n",
    "* `train.to_csv(\"./data/train.csv\", index=False)`：将训练集保存到文件 \"train.csv\"，不保存索引信息。\n",
    "* `test.to_csv(\"./data/test.csv\", index=False)`：将测试集保存到文件 \"test.csv\"，不保存索引信息。\n",
    "* `data.article.str.len().max()`： 计算所有文本内容的最大长度，并将其存储在 MAX_LEN 变量中。\n",
    "\n",
    "\n",
    "这段代码为后续的模型训练和测试做准备工作，将原始数据整理成可以被模型使用的格式，并存储到相应的文件中。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f1aa12",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c70edd-49ec-4deb-b559-ac8fac39cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(batch_size, training_dir, filename): \n",
    "    # 记录日志信息。\n",
    "    logger.info(\"Get data loader\") \n",
    "\n",
    "    # 加载预训练的 BERT tokenizer。\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True) \n",
    "\n",
    "    # 从指定的目录和文件名加载数据。\n",
    "    dataset = pd.read_csv(os.path.join(training_dir, filename)) \n",
    "\n",
    "    # 获取所有文本内容和情感标签。\n",
    "    articles = dataset.article.values \n",
    "    sentiments = dataset.sentiment.values \n",
    "\n",
    "    # 初始化一个列表，用于存储所有文本的 token id。\n",
    "    input_ids = [] \n",
    "\n",
    "    # 遍历所有文本，将其转换成 token id 并添加到列表中。\n",
    "    for sent in articles: \n",
    "        encoded_articles = tokenizer.encode(sent, add_special_tokens=True) \n",
    "        input_ids.append(encoded_articles) \n",
    "\n",
    "    # 对较短的文本进行填充，使其长度与最长的文本长度一致。\n",
    "    input_ids_padded = [] \n",
    "    for i in input_ids: \n",
    "        while len(i) < MAX_LEN: \n",
    "            i.append(0) \n",
    "        input_ids_padded.append(i) \n",
    "    input_ids = input_ids_padded \n",
    "\n",
    "    # 创建一个列表，用于存储所有文本的注意力掩码。\n",
    "    # 注意力掩码用于标识文本中的有效 token，在 BERT 模型中用于控制注意力机制。\n",
    "    attention_masks = [] \n",
    "\n",
    "    # 遍历所有 token id，生成相应的注意力掩码。\n",
    "    # 如果 token id 大于 0，则表示该 token 是有效的，对应掩码值为 1，反之则为 0。\n",
    "    for sent in input_ids: \n",
    "        att_mask = [int(token_id > 0) for token_id in sent] \n",
    "        attention_masks.append(att_mask) \n",
    "\n",
    "    # 将 token id、注意力掩码和情感标签转换为 PyTorch 张量。\n",
    "    train_inputs = torch.tensor(input_ids) \n",
    "    train_labels = torch.tensor(sentiments) \n",
    "    train_masks = torch.tensor(attention_masks) \n",
    "\n",
    "    # 使用 PyTorch 的 TensorDataset 类创建数据集，并使用 DataLoader 类创建数据加载器。\n",
    "    tensor_data = TensorDataset(train_inputs, train_masks, train_labels) \n",
    "    tensor_dataloader = DataLoader(tensor_data, batch_size=batch_size) \n",
    "\n",
    "    # 返回数据加载器。\n",
    "    return tensor_dataloader "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a60b3f",
   "metadata": {},
   "source": [
    "**解释：**\n",
    "\n",
    "这段代码定义了一个名为 `get_data_loader` 的函数，该函数用于从指定路径加载数据，并将其转换为 PyTorch 数据加载器，以便用于模型训练。\n",
    "\n",
    "* **函数参数：**  该函数接受三个参数：`batch_size` 用于指定每个 batch 中包含的样本数量，`training_dir` 用于指定存储数据的目录，`filename` 用于指定数据文件名。\n",
    "* **加载数据：**  函数首先从指定的目录和文件名中加载 CSV 数据，并获取所有文本内容和情感标签。\n",
    "* **预处理数据：**\n",
    "    * 首先使用 BERT tokenizer 将每个文本转换成 token id。\n",
    "    * 然后对较短的文本进行填充，使其长度与最长的文本长度一致，以便使用 BERT 模型进行处理。\n",
    "    * 最后生成注意力掩码，用于标识文本中的有效 token，以便 BERT 模型能够专注于处理有效的部分。\n",
    "* **创建数据集和数据加载器：**  将 token id、注意力掩码和情感标签转换成 PyTorch 张量，并使用 `TensorDataset` 类创建数据集。最后使用 `DataLoader` 类创建数据加载器，以便将数据以 batch 的形式加载到模型中，用于训练。\n",
    "\n",
    "这段代码示例了如何使用 Hugging Face 提供的 BERT 工具包和 PyTorch 库对文本数据进行预处理和数据加载，以便将数据有效地使用在模型训练中。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bccd277",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec26649c-cb7f-484e-a7da-2e70664d2d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args): \n",
    "    # 判断是否使用 GPU 进行训练。\n",
    "    use_cuda = args.num_gpus > 0 \n",
    "\n",
    "    # 设置训练设备，如果使用 GPU 则使用 CUDA，否则使用 CPU。\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\") \n",
    "\n",
    "    # 设置随机数种子，以确保每次训练结果一致。\n",
    "    torch.manual_seed(args.seed) \n",
    "    if use_cuda: \n",
    "        torch.cuda.manual_seed(args.seed) \n",
    "\n",
    "    # 加载训练数据集和测试数据集。\n",
    "    train_loader = get_data_loader(args.batch_size, args.data_dir, args.train_file) \n",
    "    test_loader = get_data_loader(args.test_batch_size, args.data_dir, args.test_file) \n",
    "\n",
    "    # 加载预训练的 BERT 模型，并设置模型参数。\n",
    "    model = BertForSequenceClassification.from_pretrained( \n",
    "        \"bert-base-uncased\",   \n",
    "        num_labels=args.num_labels,  \n",
    "        output_attentions=False,   \n",
    "        output_hidden_states=False,  ) \n",
    "\n",
    "    # 将模型加载到指定设备上。\n",
    "    model = model.to(device) # load the model to the right device \n",
    "\n",
    "    # 配置优化器。\n",
    "    optimizer = AdamW( \n",
    "        model.parameters(), \n",
    "        lr=args.lr,  # learning rate  \n",
    "    ) \n",
    "\n",
    "    # 开始训练循环。\n",
    "    for epoch in range(1, args.epochs + 1): \n",
    "        # 初始化本轮训练的累计损失为 0。\n",
    "        total_loss = 0 \n",
    "\n",
    "        # 将模型设置为训练模式。\n",
    "        model.train() \n",
    "\n",
    "        # 遍历训练数据加载器中的每个 batch 的数据。\n",
    "        for step, batch in enumerate(train_loader): \n",
    "            # 将 batch 数据加载到指定设备上。\n",
    "            b_input_ids = batch[0].to(device) \n",
    "            b_input_mask = batch[1].to(device) \n",
    "            b_labels = batch[2].to(device) \n",
    "\n",
    "            # 清除模型参数的梯度信息。\n",
    "            model.zero_grad() \n",
    "\n",
    "            # 将数据输入模型进行训练。\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels) \n",
    "\n",
    "            # 获取损失值。\n",
    "            loss = outputs[0] \n",
    "\n",
    "            # 将当前 batch 的损失值累加到 total_loss 中。\n",
    "            total_loss += loss.item() \n",
    "\n",
    "            # 反向传播计算损失函数对模型参数的梯度。\n",
    "            loss.backward() \n",
    "\n",
    "            # 进行梯度裁剪，防止梯度爆炸。\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n",
    "\n",
    "            # 根据梯度信息更新模型参数，并进行一步优化。\n",
    "            optimizer.step() \n",
    "\n",
    "            # 每隔一定步数打印训练信息。\n",
    "            if step % args.log_interval == 0: \n",
    "                logger.info( \n",
    "                    \"训练周期: {} [{}/{} ({:.0f}%)] 损失: {:.6f}\".format( \n",
    "                        epoch, \n",
    "                        step * len(batch[0]), \n",
    "                        len(train_loader.sampler), \n",
    "                        100.0 * step / len(train_loader), \n",
    "                        loss.item(), \n",
    "                    ) \n",
    "                ) \n",
    "\n",
    "        # 打印本轮训练的平均损失。\n",
    "        logger.info(\"平均训练损失: %f\\n\", total_loss / len(train_loader))  \n",
    "\n",
    "        # 在测试集上评估模型。\n",
    "        test(model, test_loader, device) \n",
    "\n",
    "        # 保存训练好的模型。\n",
    "        logger.info(\"保存训练好的模型.\") \n",
    "        model_2_save = model.module if hasattr(model, \"module\") else model \n",
    "        model_2_save.save_pretrained(save_directory=args.model_dir) \n",
    "\n",
    "        # 返回训练好的模型。\n",
    "        return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bc35a5",
   "metadata": {},
   "source": [
    "**解释：**\n",
    "\n",
    "这段代码定义了一个名为 `train` 的函数，该函数用于训练一个 BERT 模型，并使用训练好的模型在测试集上进行评估，最后将训练好的模型保存到指定目录。\n",
    "\n",
    "* **函数参数：** 函数接收一些参数，包括训练轮数、学习率、批次大小、数据路径、日志间隔等等，这些参数用于控制训练过程。\n",
    "* **初始化：** 函数首先根据参数设置训练设备（CPU 或 GPU）、随机数种子和训练数据集和测试数据集。\n",
    "* **加载模型：** 函数加载预训练的 BERT 模型，并设置模型参数，包括标签数量等。\n",
    "* **优化器：** 函数使用 AdamW 优化器进行模型参数的更新。\n",
    "* **训练循环：** 函数使用循环遍历训练数据集，并进行以下操作：\n",
    "    * 清除梯度信息\n",
    "    * 将数据输入模型进行训练\n",
    "    * 计算损失值\n",
    "    * 反向传播计算梯度\n",
    "    * 梯度裁剪（防止梯度爆炸）\n",
    "    * 更新模型参数\n",
    "    * 打印训练信息\n",
    "* **评估模型：** 函数在每个 epoch 结束后，使用训练好的模型在测试集上进行评估，并打印评估结果。\n",
    "* **保存模型：** 函数最后将训练好的模型保存到指定的目录中。\n",
    "\n",
    "这段代码展示了如何使用 BERT 模型进行文本分类任务的训练，以及如何使用 PyTorch 进行模型训练和评估。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48b59ab",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8730687b-56ed-4ba1-8172-4ddde07d5959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):     \n",
    "\n",
    "    # 定义一个函数，用于计算预测结果与真实标签之间的准确率。\n",
    "    def get_correct_count(preds, labels): \n",
    "        # 将预测结果和真实标签转换成一维数组。\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten() \n",
    "        labels_flat = labels.flatten() \n",
    "        # 计算预测正确的数量。\n",
    "        return np.sum(pred_flat == labels_flat), len(labels_flat) \n",
    "\n",
    "    # 将模型设置为评估模式。\n",
    "    model.eval() \n",
    "\n",
    "    # 初始化准确率和样本数量为 0。\n",
    "    _, eval_accuracy = 0, 0 \n",
    "    total_correct = 0 \n",
    "    total_count = 0 \n",
    "\n",
    "    # 使用 torch.no_grad() 上下文管理器来禁用梯度计算，因为在评估过程中不需要计算梯度。\n",
    "    with torch.no_grad(): \n",
    "        # 遍历测试数据集的每个 batch。\n",
    "        for batch in test_loader: \n",
    "            # 将 batch 数据加载到指定设备上。\n",
    "            b_input_ids = batch[0].to(device) \n",
    "            b_input_mask = batch[1].to(device) \n",
    "            b_labels = batch[2].to(device) \n",
    "\n",
    "            # 使用模型进行预测。\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) \n",
    "            preds = outputs[0] \n",
    "            # 将预测结果转换为 numpy 数组。\n",
    "            preds = preds.detach().cpu().numpy() \n",
    "            label_ids = b_labels.to(\"cpu\").numpy() \n",
    "\n",
    "            # 使用 get_correct_count 函数计算当前 batch 的准确率。\n",
    "            num_correct, num_count = get_correct_count(preds, label_ids) \n",
    "            # 累加预测正确的数量和样本数量。\n",
    "            total_correct += num_correct \n",
    "            total_count += num_count \n",
    "\n",
    "    # 打印测试集上的准确率。\n",
    "    logger.info(\"测试集: 准确率: %f\\n\", total_correct/total_count) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d3a83",
   "metadata": {},
   "source": [
    "**解释：**\n",
    "\n",
    "这段代码定义了一个名为 `test` 的函数，用于测试训练好的 BERT 模型，并计算其在测试集上的准确率。\n",
    "\n",
    "* **函数参数：**  函数接收三个参数：`model` 为训练好的模型，`test_loader` 为测试数据集的加载器，`device` 用于指定使用的设备（CPU 或 GPU）。\n",
    "* **计算准确率函数：**  定义了一个名为 `get_correct_count` 的子函数，用于计算预测结果和真实标签之间的准确率。\n",
    "* **模型评估：**\n",
    "    * 函数首先将模型设置为评估模式，禁用梯度计算。\n",
    "    * 然后遍历测试数据集，并使用模型进行预测。\n",
    "    * 获取预测结果和真实标签，计算准确率。\n",
    "    * 最后打印测试集上的准确率。\n",
    "\n",
    "这段代码示例了如何使用 PyTorch 对 BERT 模型进行评估，它提供了计算准确率的函数，并使用 `torch.no_grad()` 上下文管理器来禁用梯度计算，提高评估效率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b63ddf",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f9d35c-f338-4d07-80ab-fe07657ff82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个命名空间对象，用于存储训练模型需要的参数。\n",
    "args = SimpleNamespace(num_labels=3, batch_size=16, test_batch_size=10, epochs=3, lr=2e-5, seed=1,log_interval =50, model_dir = \"model/\", data_dir=\"data/\", num_gpus=1, train_file = \"train.csv\", test_file=\"test.csv\")      \n",
    "\n",
    "# 使用 train 函数训练模型。\n",
    "model = train(args) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6b087c",
   "metadata": {},
   "source": [
    "**解释：**\n",
    "\n",
    "这段代码设置了模型训练所需的各种参数，并调用 `train` 函数进行模型训练。\n",
    "\n",
    "1. **设置训练参数：**  使用 SimpleNamespace 创建一个命名空间对象 `args`，并设置其属性，用于存储模型训练所需的各种参数：\n",
    "    * **`num_labels`：**  模型需要预测的类别数量。\n",
    "    * **`batch_size`：**  训练数据的批次大小，指定每次训练时加载到模型中的样本数量。\n",
    "    * **`test_batch_size`：**  测试数据的批次大小，指定每次测试时加载到模型中的样本数量。\n",
    "    * **`epochs`：**  训练的轮数。\n",
    "    * **`lr`：**  学习率，控制模型参数更新的速度。\n",
    "    * **`seed`：**  随机数种子，确保每次训练结果的一致性。 \n",
    "    * **`log_interval`：**  日志打印间隔，指定每隔多少步打印一次训练信息。\n",
    "    * **`model_dir`：**  存储训练好的模型的目录。\n",
    "    * **`data_dir`：**  存储训练数据和测试数据的目录。\n",
    "    * **`num_gpus`：**  使用的 GPU 数量。\n",
    "    * **`train_file`：**  训练数据集的文件名。\n",
    "    * **`test_file`：**  测试数据集的文件名。\n",
    "\n",
    "2. **训练模型：** 调用 `train` 函数，并传入设置好的参数 `args`，开始模型的训练过程。\n",
    "\n",
    "这段代码展示了如何使用命名空间对象组织模型训练参数，并调用 `train` 函数启动模型训练，为后续的自然语言处理和模型应用搭建了基础。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e3eff",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230b13d8-d09f-48b6-8537-b9a33ae92ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(request_body, request_content_type): \n",
    "    # 检查请求内容类型是否为 JSON 格式。\n",
    "    if request_content_type == \"application/json\": \n",
    "        # 解析 JSON 请求体。\n",
    "        data = json.loads(request_body)     \n",
    "\n",
    "        # 检查解析后的数据类型是否为字符串或者列表。\n",
    "        if isinstance(data, str): \n",
    "            # 如果数据类型为字符串，则将其封装成一个元素的列表。\n",
    "            data = [data] \n",
    "\n",
    "        elif isinstance(data, list) and len(data) > 0 and isinstance(data[0], str): \n",
    "            # 如果数据类型为非空列表，且列表元素为字符串，则保持原样。\n",
    "            pass \n",
    "\n",
    "        else: \n",
    "            # 抛出异常，因为数据类型不支持。\n",
    "            raise ValueError(\"不支持的输入类型。输入类型必须为字符串或非空列表。我得到了 {}。\".format(data)) \n",
    "\n",
    "        # 加载预训练的 BERT tokenizer。\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True) \n",
    "\n",
    "        # 将文本数据转换成 token id。\n",
    "        input_ids = [tokenizer.encode(x, add_special_tokens=True) for x in data] \n",
    "\n",
    "        # 对较短的文本进行填充，使其长度与最长的文本长度一致。\n",
    "        padded =  torch.zeros(len(input_ids), MAX_LEN)  \n",
    "        for i, p in enumerate(input_ids): \n",
    "            padded[i, :len(p)] = torch.tensor(p) \n",
    "\n",
    "        # 创建注意力掩码，用于标识文本中的有效 token。\n",
    "        mask = (padded != 0) \n",
    "\n",
    "        # 返回 token id 和注意力掩码，用于后续模型预测。\n",
    "        return padded.long(), mask.long() \n",
    "\n",
    "    # 抛出异常，因为不支持当前的内容类型。\n",
    "    raise ValueError(\"不支持的内容类型: {}\".format(request_content_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b6d95a",
   "metadata": {},
   "source": [
    "**解释：**\n",
    "\n",
    "这段代码定义了一个名为 `input_fn` 的函数，用于处理来自模型端点的请求，并将其转换成模型可以接受的输入数据。\n",
    "\n",
    "* **函数参数：** 该函数接受两个参数：`request_body` 表示请求的正文，`request_content_type` 表示请求的内容类型。\n",
    "* **检查内容类型：**  函数首先检查请求内容类型是否为 JSON 格式。\n",
    "* **解析数据：**  如果内容类型为 JSON 格式，则解析请求正文，并将数据转换成字符串列表或单个字符串。\n",
    "* **预处理数据：**  使用预训练的 BERT tokenizer 将文本转换成 token id，并对较短的文本进行填充，使其长度与最长的文本长度一致。最后创建注意力掩码，用于标识文本中的有效 token。\n",
    "* **返回结果：**  函数返回处理后的 token id 和注意力掩码，用于后续的模型预测。\n",
    "\n",
    "这段代码展示了如何处理模型端点的请求，并使用 BERT tokenizer 对文本进行预处理，以便将数据有效地使用在模型预测中。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ddacf",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3be70d-82b6-4c85-9027-bf447cd4dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(input_data, model): \n",
    "    # 设置预测设备，如果 GPU 可用则使用 CUDA，否则使用 CPU。\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "    # 将模型加载到指定设备上。\n",
    "    model.to(device) \n",
    "\n",
    "    # 将模型设置为评估模式。\n",
    "    model.eval() \n",
    "\n",
    "    # 从输入数据中获取 token id 和注意力掩码。\n",
    "    input_id, input_mask = input_data \n",
    "\n",
    "    # 将 token id 和注意力掩码加载到指定设备上。\n",
    "    input_id = input_id.to(device) \n",
    "    input_mask = input_mask.to(device) \n",
    "\n",
    "    # 禁用梯度计算，因为在预测过程中不需要计算梯度。\n",
    "    with torch.no_grad(): \n",
    "        # 使用模型进行预测。\n",
    "        y = model(input_id, attention_mask=input_mask)[0] \n",
    "\n",
    "    # 返回预测结果。\n",
    "    return y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2975083e",
   "metadata": {},
   "source": [
    "**解释：**\n",
    "\n",
    "这段代码定义了一个名为 `predict_fn` 的函数，用于使用训练好的 BERT 模型对输入数据进行预测，并返回预测结果。\n",
    "\n",
    "* **函数参数：**  函数接收两个参数：`input_data` 表示预处理后的输入数据，`model` 表示训练好的模型。\n",
    "* **模型预测：**  函数首先将模型加载到指定的设备上，然后将模型设置为评估模式。接着，函数接收预处理后的输入数据，包括 token id 和注意力掩码，并将它们加载到指定的设备上。然后，函数使用训练好的模型对输入数据进行预测，并返回预测结果。\n",
    "\n",
    "这段代码展示了如何使用 PyTorch 和训练好的 BERT 模型进行预测。它将模型加载到合适的设备上，并禁用梯度计算，以提高预测效率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a890d775",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f4c2e-44ff-4f33-bd8f-22d910958bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 json 库，用于处理 JSON 格式数据。\n",
    "import json\n",
    "\n",
    "# 定义一个测试文本。\n",
    "article = \"Operating profit outpaced the industry average\" \n",
    "\n",
    "# 将测试文本转换为 JSON 格式字符串。\n",
    "request_body = json.dumps(article) \n",
    "\n",
    "# 使用 input_fn 函数对文本进行预处理，并获取 token id 和注意力掩码。\n",
    "enc_data, mask = input_fn(request_body, 'application/json') \n",
    "\n",
    "# 使用模型对预处理后的数据进行预测。\n",
    "output = predict_fn((enc_data, mask), model) \n",
    "\n",
    "# 将预测结果转换为 numpy 数组。\n",
    "preds = output.detach().cpu().numpy() \n",
    "\n",
    "# 打印预测结果，即情感标签。\n",
    "print(\"情感标签: \" + str(np.argmax(preds))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff2e721",
   "metadata": {},
   "source": [
    "**解释：**\n",
    "\n",
    "这段代码演示了如何使用训练好的模型对新的文本数据进行情感分类预测。\n",
    "\n",
    "* **准备输入数据：** 首先定义一个测试文本 `article`，并使用 `json.dumps` 将其转换为 JSON 格式字符串，模拟模型部署后的请求格式。\n",
    "* **数据预处理：**  使用 `input_fn` 函数对 JSON 格式的文本数据进行预处理，将其转换为模型可以接受的输入数据，包括 token id 和 attention mask。\n",
    "* **模型预测：** 使用 `predict_fn` 函数加载训练好的模型，并对预处理后的数据进行预测，得到预测结果 `output`。\n",
    "* **结果处理：** 将预测结果 `output` 转换为 NumPy 数组，并使用 `np.argmax` 获取预测结果中概率最大的类别索引，该索引即为模型预测的情感标签。最后打印预测的情感标签。\n",
    "\n",
    "这段代码展示了如何使用训练好的 BERT 模型对新的文本数据进行情感分类预测，并展示了完整的预测流程，包括数据预处理、模型预测和结果处理。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879633a5",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
